\chapter*{Abstract}
\glsresetall
As Transformer(-based) architectures in the field of \gls{nlp} have gotten increasingly better, so have they gotten increasingly bigger. These \gls{nn} architectures often-times contain hundreds of millions of -- and more recently hundreds of billions of -- parameters, which hurts their sustainability and accessibility. It is no surprise then, that methods which aim at making these \glspl{nn} more efficient have become increasingly popular. \gls{kd} is just one such method, in which a smaller, untrained student network is trained to mimic a larger, (pre-)trained teacher network. Research into \gls{kd} as applied to Transformer(-based) architectures often-times use huge corpora, without any scientific foundation. This makes the \gls{kd} process itself quite inefficient, ironically enough.

This thesis investigates the effects of the transfer dataset $\dataset_T$ used for the \gls{kd} process, both in terms of \emph{size} and in terms of \emph{composition}. To this end, we distill the knowledge of \bertbase, our teacher network of choice, into our student network of choice, \bertstudent, multiple times using transfer datasets of various sizes and compositions. After distillation we evaluate each distilled student network on the GLUE benchmark and on SQuAD.

\todo{Rewrite the abstract to make it more "glass half full" (i.e. no "mixed results"). Emphasize efficiency/effectiveness trade-off.}
Our investigation has yielded mixed results. When it comes to the \emph{size} of transfer dataset $\dataset_T$, we show that it is possible to successfully distill the knowledge of the teacher into the student using a transfer dataset that is a fraction of the size of what is conventionally used. This all at marginal (if not negligible) cost in terms of downstream task performance. With respect to the \emph{composition} of transfer dataset $\dataset_T$, we show that randomizing and/or generating sequences does come at a significant cost in terms of performance.